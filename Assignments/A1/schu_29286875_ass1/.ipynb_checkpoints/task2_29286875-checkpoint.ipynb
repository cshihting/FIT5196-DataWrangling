{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1: Task 2: Text Pre-Processing (%30)\n",
    "#### Student Name:  Shih Ting Chu\n",
    "#### Student ID:  29286875\n",
    "\n",
    "Date: Aug 15\n",
    "\n",
    "Environment: Python 3.6.3 and Jupyter notebook\n",
    "Libraries used: \n",
    "* os (Miscellaneous operating system interfaces, e.g. os.listdir, for finding files by path)\n",
    "* re (Regular expression operations)\n",
    "* nltk (Natural Language Toolkit, e.g. tokenizer, lemmatizer, stopwords, collocations and probabilities)\n",
    "* OrderedDict (A dictionary subclass to remember the order in which its contents are added)\n",
    "\n",
    "\n",
    "### 1. Introduction\n",
    "This assessment touches on the next step of analyzing textual data, i.e., converting the extracted data into a proper format. <br>In this assessment, you are required to write Python code to preprocess a set of resumes and convert them into numerical <br> representations (which are suitable for input into recommender-systems/ information-retrieval algorithms).\n",
    "The data-set <br> that we provide contains 250 CVs for each student. Please find the  resume\\_dataset.txt to know the PDF files in your own <br> data-set. Each line in the csv file contains the id of the resumes that a student needs to include in the data-set ( for example <br> 1111111111: [3 34 5 ...] means that the student 1111111111 data-set includes resume\\_(3), resume\\_(34), resume\\_(5),... ). <br> CVs contain information about the applicants represented in the PDF format.\n",
    "The information includes, for example, personal <br> information, skills, work experience, education, etc. Your task is to extract and transform the information for each applicant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eileen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import miscellaneous operating system interfaces\n",
    "import os\n",
    "# library for regular expression\n",
    "import re\n",
    "# Import RegexpTokenizer (比較聰明的抓字)\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Import PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "# Extracting from a text a list of n-gram can be easily accomplished with function ngram()\n",
    "from nltk.util import ngrams\n",
    "# NLTK provides a built-in function FreqDist to compute this distribution directly from a set of word tokens.\n",
    "from nltk.probability import *\n",
    "\n",
    "# Import libraries for bigrams analysis\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.util import bigrams\n",
    "from nltk import BigramCollocationFinder\n",
    "\n",
    "# Sort dictionary in ascending/descending order based on values\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get My Selected Resume Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n"
     ]
    }
   ],
   "source": [
    "# Dataset for me(29286875) copied from moodle\n",
    "myDataset = '262 76 778 565 6 623 \\\n",
    "200 742 645 418 520 814 467 680 216 12 762 63 \\\n",
    "276 66 236 722 82 779 626 206 276 649 750 20 167 738 602 6 251 808 \\\n",
    "729 115 80 71 539 38 426 44 556 779 251 760 61 728 251 268 421 750 \\\n",
    "832 807 55 452 683 303 560 555 547 265 649 441 633 108 517 744 137 \\\n",
    "446 182 397 442 8 266 630 857 17 498 32 293 711 413 445 392 606 322 \\\n",
    "422 54 582 657 239 309 382 286 105 336 786 788 712 738 516 29 523 \\\n",
    "51 428 43 47 819 649 598 297 615 208 865 720 122 789 372 860 813 \\\n",
    "219 700 275 4 466 455 277 427 728 71 812 284 646 327 397 262 22 \\\n",
    "450 80 377 26 332 45 325 24 274 153 847 311 168 99 361 362 139 602 \\\n",
    "551 517 111 812 1 364 467 323 398 207 837 716 733 685 498 817 336 \\\n",
    "395 154 265 492 117 389 833 705 703 270 218 435 475 338 513 621 \\\n",
    "208 839 560 370 81 445 89 12 645 336 157 47 91 32 797 837 803 195 \\\n",
    "861 237 31 810 634 211 246 278 788 556 726 684 815 460 596 237 640 \\\n",
    "764 251 304 474 154 384 743 749 593 327 749 515 543 310 833 464 724 \\\n",
    "66 331 649 397 426 383 492'\n",
    "\n",
    "# Create an empty list\n",
    "myDataset_ls = []\n",
    "\n",
    "# Convert string to list\n",
    "myDataset_ls = myDataset.split(' ')\n",
    "\n",
    "# Remove the duplicate values\n",
    "clean_myDataset_ls = list(set(myDataset_ls))\n",
    "\n",
    "# Format filenames assign to cv_name_ls\n",
    "cv_name_ls = ['resume_(' + s + ').txt' for s in clean_myDataset_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Function for Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract bigrams ()\n",
    "def generate_collocations(tokens):\n",
    "    '''\n",
    "    Parameter: given list of tokens\n",
    "    Return: bigrams\n",
    "    '''\n",
    "\n",
    "    # Create an empty list to store stopwords\n",
    "    stopwords_list = []\n",
    "    # Open and read the stopword file, then add into stopwords_list\n",
    "    with open('stopwords_en.txt') as f:\n",
    "        stopwords_list = f.read().splitlines()\n",
    "    # Score words \n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "    # Best results with window_size\n",
    "    finder = BigramCollocationFinder.from_words(tokens, window_size = 2)\n",
    "    # The token is stopword (the length is less than 3 or the lowercase)\n",
    "    finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in stopwords_list)\n",
    "    # The token just occurs once\n",
    "    finder.apply_freq_filter(1)\n",
    "\n",
    "    # The best 20 bigrams\n",
    "    collocations = finder.nbest(bigram_measures.likelihood_ratio, 20)\n",
    "\n",
    "    return collocations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Function for Tokens Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze(resume, stopword):    \n",
    "    \n",
    "    \n",
    "    '''Word tokenization'''\n",
    "    token_regex = r\"\\w+(?:[-']\\w+)?\"\n",
    "    # Find the matched token in clean_string\n",
    "    match_token = re.findall(token_regex, resume)\n",
    "    \n",
    "    # Create a list to store alphabet tokens\n",
    "    tokens = []   \n",
    "    for each in match_token:\n",
    "        if each.isalpha():\n",
    "            tokens.append(each)\n",
    "   \n",
    "\n",
    "    '''Keep only capital words which are in the middle of sentences'''\n",
    "    # Set regex for getting the first word in a sentence\n",
    "    first_regex = r'(?:(?:[^\\w,]\\s)|(?:\\.\\s))([A-Z]\\w+)\\s'\n",
    "    # Find the first token in each sentence\n",
    "    match_first = re.findall(first_regex, resume)\n",
    "    \n",
    "    # Create empty lists\n",
    "    mix = [] # Store lowercase & uppercase(in the middle of lines) tokens \n",
    "    upper = [] # Store those capital tokens in the middle of sentences\n",
    "    gonna_stem = [] # mix - upper (\"stem\" will make all tokens lowercase so need to do it separately)\n",
    "        \n",
    "    # Check each token in match_token\n",
    "    for each in tokens:\n",
    "        # If that token is the first word in each sentence\n",
    "        if each in first_regex:\n",
    "            # Make it lowercase and then add into mix\n",
    "            mix.append(each.lower())            \n",
    "        else:\n",
    "            # Add into vocab\n",
    "            mix.append(each)\n",
    "    \n",
    "    # Split uppercase tokens and lowercase tokens\n",
    "    for each in mix:\n",
    "        if each.istitle():\n",
    "            upper.append(each)\n",
    "        else:\n",
    "            gonna_stem.append(each)    \n",
    "\n",
    "            \n",
    "    '''Stem'''\n",
    "    # Create a new Porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    # Stem tokens using the Porter stemmer\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in gonna_stem]\n",
    "    \n",
    "    # Combine stemmed_tokens and upper (stemmed lowercase tokens & capital tokens in the middle lines)\n",
    "    vocab = []\n",
    "    # Make each token unique\n",
    "    vocab = list(set(stemmed_tokens + upper))\n",
    "\n",
    "\n",
    "    '''Remove stopwords'''\n",
    "    # Create an empty list to store stopwords\n",
    "    no_stopwords_list = []\n",
    "\n",
    "    # Open and read the stopword file, then add into stopwords_list\n",
    "    with open(stopword) as f:\n",
    "        no_stopwords_list = f.read().splitlines()\n",
    "    \n",
    "    # Create a tokens list without stopwords\n",
    "    filtered_tokens = [each for each in vocab if each not in no_stopwords_list]\n",
    "\n",
    "\n",
    "    '''Remove tokens with the length less than 3'''\n",
    "    # Create an empty list\n",
    "    no_less3_tokens = []\n",
    "\n",
    "    # Check each token in sorted_often_tokens\n",
    "    for each in filtered_tokens:\n",
    "        # If the length of each token >= 3\n",
    "        if len(each) >= 3:\n",
    "            # Append that token into no_less3_tokens\n",
    "            no_less3_tokens.append(each)\n",
    "\n",
    "\n",
    "    '''Remove rare tokens (with the threshold set to %2)'''\n",
    "    # Create a dictionary to store tokens and count their frequency \n",
    "    token_freq = {}\n",
    "    # Check each token in all resumes\n",
    "    for each in tokens:\n",
    "        # If each token is in no_less3_tokens (list)\n",
    "        if each in no_less3_tokens:\n",
    "            # If each token is not in token_freq (dict)\n",
    "            if each not in token_freq:\n",
    "                # Add key into token_freq\n",
    "                token_freq[each] = 0 \n",
    "            # Count the token    \n",
    "            token_freq[each] += 1\n",
    "\n",
    "    # Sort tokens frequency in a dictionary in ascending order\n",
    "    ascending_dict = OrderedDict(sorted(token_freq.items(), key=lambda key: key[1]))\n",
    "    # Convert dictionary to list\n",
    "    ascending_list = list(ascending_dict.items())\n",
    "    # Remove rare tokens (2% of total tokens)\n",
    "    no_rare_tokens = ascending_list[int(len(ascending_list)*0.02):]\n",
    "    \n",
    "    # Create a list to store only tokens but their frequency\n",
    "    clean_uni_tokens = []\n",
    "    for each in no_rare_tokens:\n",
    "        clean_uni_tokens.append(each[0])\n",
    "\n",
    "\n",
    "    '''First 200 meaningful bigrams'''\n",
    "    # Get the bigrams list like [('I', 'am'), ('good', 'day'), ...]\n",
    "    bigram_ls = generate_collocations(vocab)\n",
    "    \n",
    "    # Create an empty list\n",
    "    split_ls = []\n",
    "    # Get all values from bigram_ls and store into split_ls\n",
    "    # Index of each bigram \n",
    "    for bigram in range(len(bigram_ls)):\n",
    "        # Index of each word of a bigram\n",
    "        for word in range(len(bigram_ls[bigram])):\n",
    "            # Append into split_ls\n",
    "            split_ls.append(bigram_ls[bigram][word])\n",
    "    \n",
    "    # Create an empty list\n",
    "    best200 = []\n",
    "    # Make it look like [('I am'), ('good day'), ...]\n",
    "    for each in range(0, len(split_ls), 2):\n",
    "        best200.append(split_ls[each]+\" \"+split_ls[each+1])\n",
    "        \n",
    "    \n",
    "    ''' Vocab (token_string:integer_index) '''\n",
    "    final_tokens = sorted(set(clean_uni_tokens+best200))\n",
    "    \n",
    "    vocab_str = ''\n",
    "    # Make it readable\n",
    "    for integer_index, token_string in enumerate(final_tokens, 1):\n",
    "        vocab_str += token_string + \": \" + str(integer_index) + \"\\n\"\n",
    "        \n",
    "    # Write into a file\n",
    "    f = open('29286875_vocab.txt','a+')\n",
    "    f.write(vocab_str)\n",
    "    f.close()  \n",
    "    \n",
    "    \n",
    "    ''' CountVec (file_name, token_index:count, token_index:count,...) '''\n",
    "    token_index = []\n",
    "    for num in range(1, len(final_tokens)+1):\n",
    "        token_index.append(num)\n",
    "    \n",
    "    token_dict = dict(zip(final_tokens, token_index))\n",
    "    \n",
    "    \n",
    "    # Create a string to store per resume\n",
    "    each_resume = ''\n",
    "    # Return a list containing the names of the entries in the directory given by path\n",
    "    # os.listdir(path of resumeTxt)\n",
    "    for file in os.listdir(\"/Users/eileen/Jupyter/5196 data wrangling/Assignments/A1/PDF/resumeTxt+task2_pdf\"):\n",
    "        check_ls = []\n",
    "        # If file is my selected resume\n",
    "        if file in cv_name_ls:\n",
    "            # Read each resume\n",
    "            with open(file, 'r') as f:\n",
    "                # Read line by line\n",
    "                for line in f.readlines():\n",
    "                    clean_line = line.strip()\n",
    "                    # Add into each_resume\n",
    "                    each_resume += clean_line\n",
    "                check_ls = each_resume.split(\" \")\n",
    "\n",
    "            count_ls = []    \n",
    "            # Check each token in all resumes, check_ls(messy tokens)\n",
    "            for each in list(set(check_ls)):\n",
    "                # If the token is in final_tokens(clean tokens)\n",
    "                if each in final_tokens:\n",
    "                    count_str = ''\n",
    "                    \n",
    "                    # Get the index of the token and count its frequency\n",
    "                    count_str = str(token_dict[each]) + \": \" + str(check_ls.count(each))\n",
    "                    count_ls.append(count_str)\n",
    "\n",
    "            # Write into a file\n",
    "            f = open('29286875_countVec.txt','a+')\n",
    "            f.write(\"\\n\\n\")\n",
    "            f.write(file)\n",
    "            f.write(\":\\n\")\n",
    "            for item in count_ls:\n",
    "                f.write(\"%s, \" % item)\n",
    "            f.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Open and Read Each Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an empty string to store all my resumes\n",
    "my_CVs = ''\n",
    "\n",
    "# Return a list containing the names of the entries in the directory given by path\n",
    "# os.listdir(path of resumeTxt)\n",
    "for file in os.listdir(\"/Users/eileen/Jupyter/5196 data wrangling/Assignments/A1/PDF/resumeTxt+task2_pdf\"):\n",
    "    # If the file is in cv_name_ls\n",
    "    if file in cv_name_ls:\n",
    "        # Open and read that file from the chosen path\n",
    "        with open(file, 'r') as f:\n",
    "            # Read line by line\n",
    "            for line in f.readlines():\n",
    "                # Add into my_CVs\n",
    "                my_CVs += line\n",
    "\n",
    "# Call function to analyze PDF\n",
    "analyze(my_CVs, 'stopwords_en.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
