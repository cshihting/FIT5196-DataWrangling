{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1: Task 2: Text Pre-Processing (%30)\n",
    "#### Student Name:  Shih Ting Chu\n",
    "#### Student ID:  29286875\n",
    "\n",
    "Date: Aug 15\n",
    "\n",
    "Environment: Python 3.6.3 and Jupyter notebook\n",
    "Libraries used: \n",
    "* os (Miscellaneous operating system interfaces, e.g. os.listdir, for finding files by path)\n",
    "* re (Regular expression operations)\n",
    "* nltk (Natural Language Toolkit, e.g. tokenizer, lemmatizer, stopwords, collocations and probabilities)\n",
    "\n",
    "\n",
    "### 1. Introduction\n",
    "This assessment touches on the next step of analyzing textual data, i.e., converting the extracted data into a proper format. <br>In this assessment, you are required to write Python code to preprocess a set of resumes and convert them into numerical <br> representations (which are suitable for input into recommender-systems/ information-retrieval algorithms).\n",
    "The data-set <br> that we provide contains 250 CVs for each student. Please find the  resume\\_dataset.txt to know the PDF files in your own <br> data-set. Each line in the csv file contains the id of the resumes that a student needs to include in the data-set ( for example <br> 1111111111: [3 34 5 ...] means that the student 1111111111 data-set includes resume\\_(3), resume\\_(34), resume\\_(5),... ). <br> CVs contain information about the applicants represented in the PDF format.\n",
    "The information includes, for example, personal <br> information, skills, work experience, education, etc. Your task is to extract and transform the information for each applicant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eileen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import miscellaneous operating system interfaces\n",
    "import os\n",
    "# library for regular expression\n",
    "import re\n",
    "# Import RegexpTokenizer (比較聰明的抓字)\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Import PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "# Extracting from a text a list of n-gram can be easily accomplished with function ngram()\n",
    "from nltk.util import ngrams\n",
    "# NLTK provides a built-in function FreqDist to compute this distribution directly from a set of word tokens.\n",
    "from nltk.probability import *\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.util import bigrams\n",
    "from nltk import BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get My Selected Resume Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for me(29286875) copied from moodle\n",
    "myDataset = '262 76 778 565 6 623 200 742 645 418 520 814 467 680 216 12 762 63 \\\n",
    "276 66 236 722 82 779 626 206 276 649 750 20 167 738 602 6 251 808 \\\n",
    "729 115 80 71 539 38 426 44 556 779 251 760 61 728 251 268 421 750 \\\n",
    "832 807 55 452 683 303 560 555 547 265 649 441 633 108 517 744 137 \\\n",
    "446 182 397 442 8 266 630 857 17 498 32 293 711 413 445 392 606 322 \\\n",
    "422 54 582 657 239 309 382 286 105 336 786 788 712 738 516 29 523 \\\n",
    "51 428 43 47 819 649 598 297 615 208 865 720 122 789 372 860 813 \\\n",
    "219 700 275 4 466 455 277 427 728 71 812 284 646 327 397 262 22 \\\n",
    "450 80 377 26 332 45 325 24 274 153 847 311 168 99 361 362 139 602 \\\n",
    "551 517 111 812 1 364 467 323 398 207 837 716 733 685 498 817 336 \\\n",
    "395 154 265 492 117 389 833 705 703 270 218 435 475 338 513 621 \\\n",
    "208 839 560 370 81 445 89 12 645 336 157 47 91 32 797 837 803 195 \\\n",
    "861 237 31 810 634 211 246 278 788 556 726 684 815 460 596 237 640 \\\n",
    "764 251 304 474 154 384 743 749 593 327 749 515 543 310 833 464 724 \\\n",
    "66 331 649 397 426 383 492'\n",
    "\n",
    "# Create an empty list\n",
    "myDataset_ls = []\n",
    "\n",
    "# Convert string to list\n",
    "myDataset_ls = myDataset.split(' ')\n",
    "\n",
    "# Format filenames assign to cv_name_ls\n",
    "cv_name_ls = ['resume_(' + s + ').txt' for s in myDataset_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Function for Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract bigrams\n",
    "def generate_collocations(tokens):\n",
    "    '''\n",
    "    Given list of tokens, return collocations.\n",
    "    '''\n",
    "\n",
    "    ignored_words = nltk.corpus.stopwords.words('english')\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "    # Best results with window_size, freq_filter of: (2,1) (2,2) (5,1)\n",
    "    finder = BigramCollocationFinder.from_words(tokens, window_size = 2)\n",
    "    finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "    finder.apply_freq_filter(1)\n",
    "\n",
    "    colls = finder.nbest(bigram_measures.likelihood_ratio, 20)\n",
    "\n",
    "    return colls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Function for Tokens Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(filename, each_resume, stopword):    \n",
    "    # Set regex for each token\n",
    "    token_regex = r\"\\w+(?:[-']\\w+)?\"\n",
    "\n",
    "    # Find the matched token in clean_string\n",
    "    match_token = re.findall(token_regex, each_resume)\n",
    "    \n",
    "    # -------------轉成小寫，除了中間的capital word\n",
    "    # Get capital tokens in the middle of the sentences by using regex\n",
    "    capital_tokens_regex = r'(?:(?:[^\\w,]\\s)|(?:\\.\\s))([A-Z]\\w+)'\n",
    "    \n",
    "    # Find the first token in each sentence\n",
    "    match_capital = re.findall(capital_tokens_regex, each_resume)\n",
    "    \n",
    "    # Create an empty string\n",
    "    vocab = []\n",
    "    # Check each token in tokens\n",
    "    for token in match_token:\n",
    "        # If that token is the first token in each sentence\n",
    "        if token in match_capital:\n",
    "            # Make it lowercase and then add into vocab\n",
    "            vocab.append(token.lower())\n",
    "        else:\n",
    "            # Add into vocab\n",
    "            vocab.append(token)\n",
    "    \n",
    "#     # TEST: keep capital tokens in the middle of sentences\n",
    "#     return print(len(set(vocab)), len(set(match_token)))\n",
    "#     return print(vocab)\n",
    "        \n",
    "    # -------------移除stopword\n",
    "    # Create an empty list to store stopwords\n",
    "    stopwords_list = []\n",
    "\n",
    "    # Open and read the stopword file, then add into stopwords_list\n",
    "    with open(stopword) as f:\n",
    "        stopwords_list = f.read().splitlines()\n",
    "    \n",
    "    # Create a tokens list without stopwords\n",
    "    filtered_tokens = [token for token in vocab if token not in stopwords_list]\n",
    "    \n",
    "#     # TEST: make sure filter stopwords\n",
    "#     return print(len(set(filtered_tokens)), len(set(vocab)))\n",
    "#     return print(filtered_tokens)\n",
    "\n",
    "    # -------------stem 轉換回根源\n",
    "    stemmer = PorterStemmer()\n",
    "    # Stem tokens using the Porter stemmer\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "#     # TEST: stem tokens\n",
    "#     return print(len(set(stemmed_tokens)), len(set(filtered_tokens)))\n",
    "#     return print(stemmed_tokens)\n",
    "\n",
    "    # -------------移除三個字母以下的字\n",
    "    # Create an empty list\n",
    "    clean_uni_tokens = []\n",
    "\n",
    "    # Check each token in sorted_often_tokens\n",
    "    for each in stemmed_tokens:\n",
    "        # If the length of each token >= 3\n",
    "        if len(each) >= 3:\n",
    "            # Append that token into token_3more_len_list\n",
    "            clean_uni_tokens.append(each)\n",
    "\n",
    "#     # TEST: remove tokens less than 3 char\n",
    "#     return print(len(set(clean_uni_tokens)), len(set(stemmed_tokens)))    \n",
    "#     return print(clean_uni_tokens)\n",
    "\n",
    "    # -------------前200個有意義的bigrams\n",
    "    # Get the bigrams list like [('I', 'am'), ('good', 'day'), ...]\n",
    "    bigram_ls = generate_collocations(vocab)\n",
    "    \n",
    "    # Create an empty list\n",
    "    split_ls = []\n",
    "    # Get all values from bigram_ls and store into split_ls\n",
    "    for bigram in range(len(bigram_ls)):\n",
    "        for word in range(len(bigram_ls[bigram])):\n",
    "            split_ls.append(bigram_ls[bigram][word])\n",
    "    \n",
    "    # Create an empty list\n",
    "    best200 = []\n",
    "    # Make it look like [('I am'), ('good day'), ...]\n",
    "    for each in range(0, len(split_ls), 2):\n",
    "        best200.append(split_ls[each]+\" \"+split_ls[each+1])\n",
    "        \n",
    "#     # TEST: check best 200 bigrams    \n",
    "#     return print(best200)\n",
    "    \n",
    "#     # TEST: Analysis of tokens for each resume\n",
    "#     analysis = ''\n",
    "#     split = \"\\n\" + \"-\"*40\n",
    "#     total = \"\\nThe total number of tokens: \" + str(len(match_token))\n",
    "#     distinct = \"\\nThe total number of distinct tokens :\" + str(len(set(clean_uni_tokens)))\n",
    "#     diversity = \"\\nThe lexical diversity: \" + str(len(match_token)/len(set(clean_uni_tokens)))\n",
    "#     analysis += split + total + distinct + diversity\n",
    "#     return print(analysis)\n",
    "    \n",
    "    # -------------vocab (token_string:integer_index)\n",
    "    final_tokens = sorted(set(clean_uni_tokens+best200))\n",
    "    \n",
    "    vocab_str = ''\n",
    "    for integer_index, token_string in enumerate(final_tokens, 1):\n",
    "        vocab_str += token_string + \": \" + str(integer_index) + \"\\n\"\n",
    "        \n",
    "#     # Write into a file\n",
    "#     f = open('29286875_vocab.txt','a+')\n",
    "#     f.write(\"\\n-----------\\n\" + vocab_str + \"\\n\")\n",
    "#     f.close()    \n",
    "    \n",
    "    # 剩這裡了\n",
    "    # -------------countVec (file_name, token_index:count, token_index:count,...)\n",
    "    uni_str = ''\n",
    "    uni_ls = []\n",
    "        \n",
    "    for each in clean_uni_tokens:\n",
    "        if each in match_token:\n",
    "            uni_str = each + \":\" + str(match_token.count(each))\n",
    "            uni_ls.append(uni_str)\n",
    "    \n",
    "#     return print(uni_ls)\n",
    "    return print(filename, \":\", uni_ls)\n",
    "    \n",
    "#     unigram_diversity = set(match_token)\n",
    "#     for uni in unigram_diversity:\n",
    "#         uni_count.append(match_token.count(uni))\n",
    "        \n",
    "    \n",
    "#     bigram_diversity = set(best200)\n",
    "#     for bi in bigram_diversity:\n",
    "#         bi_count.append(match_token.count(bi))\n",
    "            \n",
    "#     return print(analysis, \"\\n\", uni_count, \"\\n\\n-\\n\")\n",
    "    \n",
    "#     # Write into a file\n",
    "#     f = open('29286875_countVec.txt','a+')\n",
    "#     f.write(vocab_str + \"\\n\")\n",
    "#     f.close() \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Open and Read Each Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume_(4).txt : ['852:1', '6748-8526:1', 'kinkeung0531:1', 'gmail:1', 'year:1', 'half:1', 'year:1', 'firm:2', 'year:1', 'fund:6', 'year:1', 'due:1', 'audit:1', 'fund:6', 'control:2', 'risk:7', '2016:2', 'process:1', 'fund:6', 'perform:1', 'benchmark:1', 'forecast:1', 'fund:6', 'risk:7', 'product:1', 'firm:2', 'wide:1', 'department:1', 'project:1', 'product:1', 'launch:1', 'scheme:1', 'merger:1', 'workflow:1', 'process:1', 'fund:6', 'team:2', 'perform:1', 'fund:6', 'process:1', 'fund:6', 'perform:1', 'price:2', 'control:2', 'special:1', 'project:1', 'team:2', 'project:1', 'system:1', '2014:1', '2016:2', '2013:3', '2013:3', 'audit:1', 'risk:7', 'process:1', 'perform:1', 'walkthrough:1', 'control:2', 'fair:1', 'market:1', 'price:2', 'risk:7', 'interest:1', 'rate:1', 'risk:7', 'credit:1', 'risk:7', 'risk:7', 'tax:2', 'tax:2', '2010:4', '2013:3', 'system:1', '2003:1', '2010:4', '2012:1', '2011:1', '2010:4', '2010:4', 'special:1', 'skill:3', 'written:2', 'spoken:2', 'written:2', 'spoken:2', 'strong:1', 'skill:3', 'skill:3', 'quick:1', 'learner:1', 'strong:1', 'skill:3', 'problem:1', 'skill:3', 'word:1', 'word:1', 'process:1', '000:2', 'month:2', '000:2', 'month:2', '1-month:1']\n",
      "resume_(8).txt : ['qyangaf:1', 'gmail:1', '852:1', '6020:1', '4161:1', 'account:1', 'account:1', '500:1', 'research:1', 'background:1', 'real:2', 'fund:2', 'account:1', 'account:1', 'research:1', 'real:2', '2014:3', '2015:2', '2010:1', '2014:3', '2015:2', 'fund:2', 'set:1', 'progress:1', 'settlement:2', 'part:1', 'system:1', 'built:1', 'process:2', 'perform:1', 'portfolio:1', 'work:1', 'risk:1', 'margin:1', 'account:1', 'settlement:2', 'audit:3', 'year-end:1', 'stock:1', 'cost:1', 'breakdown:1', 'data:1', 're-group:1', '2013:3', '2014:3', 'account:1', 'account:1', 'audit:3', 'process:2', 'account:1', 'audit:3', 'work:1', '2013:3', '2013:3']\n",
      "resume_(6).txt : ['chow:2', 'chester:2', '852:2', '9279:2', '7979:2', 'chester:2', 'chow:2', 'gmail:2', '2016:3', 'group:2', 'full:2', 'spectrum:2', 'group:2', 'review:2', 'review:2', 'review:2', 'code:1', 'conduct:1', 'local:1', 'law:1', '2013:4', '2016:3', 'review:2', 'annual:4', 'interim:2', 'pre:1', 'board:2', 'interest:1', 'concert:1', 'control:1', 'group:2', 'board:2', 'code:1', 'interest:1', 'group:2', '2013:4', '2013:4', 'group:2', 'group:2', 'share:1', 'annual:4', 'return:4', 'tax:2', 'return:4', 'bank:2', 'account:1', '2010:2', '2013:4', 'full:2', 'spectrum:2', 'review:2', 'annual:4', 'interim:2', 'review:2', 'annual:4', 'return:4', 'tax:2', 'return:4', 'bank:2', 'bank:2', 'account:1', 'ad-hoc:1', 'chow:2', 'group:2', '2012:2', '2016:3', '2018:1', '2012:2', '2015:1', '2008:1', '2010:2', '2003:2', '2005:2', '6-7:1', '2005:2', '1998:1', '2003:2', '1-5:1', 'chow:2', 'chester:2', '852:2', '9279:2', '7979:2', 'chester:2', 'chow:2', 'gmail:2', 'command:1', 'written:1', 'spoken:1', 'wpm:2', 'wpm:2', '000:2', '000:2', '1-month:1']\n",
      "resume_(1).txt : ['track-record:1', 'region:3', 'debt:2', 'portfolio:9', 'lead:1', 'asset:3', 'mutual:2', 'fund:4', 'wealth:3', 'public:1', 'fund:4', 'class:1', 'member:1', 'return:1', 'annual:1', 'past:1', 'asset:3', 'class:1', 'www:7', 'ahlibank:1', 'asset:3', 'client:1', 'region:3', 'fund:4', 'market:4', 'mutual:2', 'fund:4', 'portfolio:9', 'suit:1', 'risk:3', 'led:1', 'oversight:1', 'asset:3', 'select:1', 'rket:1', 'risk:3', 'market:4', 'debt:2', '340:1', 'debt:2', 'placement:1', '654:1', 'agent:4', '305:1', '2nd:1', 'agent:4', '248:1', 'agent:4', '129:1', 'complaint:1', 'fund:4', 'fund:4', 'agent:4', 'wealth:3', 'asset:3', 'term:1', 'return:1', 'wealth:3', 'platform:1', 'wealth:3', 'stake:3', 'stake:3', 'stake:3', 'mail:3', 'vgowribalan:6', 'gmail:3', 'vgowribalan:6', 'live:3', '971:3', '246:3', '0800:3', 'www:7', 'portfolio:9', 'member:1', 'portfolio:9', 'portfolio:9', 'purview:1', 'portfolio:9', 'portfolio:9', 'fund:4', 'market:4', 'oversight:1', 'portfolio:9', 'market:4', 'region:3', 'sector:1', 'select:1', 'return:1', 'alpha:2', 'fincorp:1', 'www:7', 'fincorp:1', 'org:1', 'portfolio:9', 'mutual:2', 'fund:4', 'asset:3', 'total:1', 'portfolio:9', 'return:1', 'mutual:2', 'fund:4', 'risk:3', 'return:1', 'alpha:2', 'excess:2', 'fund:4', '2007:2', 'excess:2', 'portfolio:9', 'return:1', 'research:1', 'lead:1', 'analyst:1', 'largest:1', 'market:4', '2007:2', 'www:7', 'mghglobal:1', 'total:1', 'portfolio:9', 'return:1', 'hnb:1', 'www:7', 'hnb:1', 'www:7', 'hnbsl:1', 'market:4', 'market:4', 'total:1', 'portfolio:9', '100:1', 'return:1', 'total:1', 'period:1', 'www:7', 'mail:3', 'vgowribalan:6', 'gmail:3', 'vgowribalan:6', 'live:3', '971:3', '246:3', '0800:3', 'return:1', 'asset:3', 'portfolio:9', 'stment:1', 'ial:2', 'fincorp:1', 'hnb:1', 'ional:1', 'hnb:1', '1st:3', '15th:1', '28th:2', '30th:2', '28th:2', '8th:1', '1st:3', '31st:1', '23rd:1', '30th:2', '1st:3', '6th:1', 'class:1', 'member:1', '134821:1', '9582697:1', 'wealth:3', 'risk:3', 'risk:3', 'ial:2', 'market:4', 'iation:1', 'market:4', 'platform:1', 'day:1', 'system:1', 'framework:1', 'market:4', '4th:1', '1981:1', 'mail:3', 'vgowribalan:6', 'gmail:3', 'vgowribalan:6', 'live:3', '971:3', '246:3', '0800:3']\n"
     ]
    }
   ],
   "source": [
    "# Import miscellaneous operating system interfaces\n",
    "import os\n",
    "\n",
    "# # Create an empty string\n",
    "# per_CV = ''\n",
    "# my_CVs = ''\n",
    "\n",
    "# Return a list containing the names of the entries in the directory given by path\n",
    "# os.listdir(path of resumeTxt)\n",
    "for file in os.listdir(\"/Users/eileen/Jupyter/5196 data wrangling/Assignments/A1/PDF\"):\n",
    "    # Create an empty string\n",
    "    per_CV = ''\n",
    "    # If the file is in cv_name_ls\n",
    "    if file in cv_name_ls:\n",
    "        # Open and read that file from the chosen path\n",
    "        with open(file, 'r') as f:\n",
    "            # Read line by line\n",
    "            for line in f.readlines():\n",
    "                # Add into my_CVs\n",
    "                per_CV += line\n",
    "#             print(per_CV)\n",
    "            # Function for analyzing\n",
    "            analyze(file, per_CV, 'stopwords_en.txt')\n",
    "\n",
    "# Write into a file (test)\n",
    "# f = open('29286875_CVs.txt','a+')\n",
    "# f.write(my_CVs)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocab.txt\n",
    "29286875_vocab.txt : It contains the bigrams and unigrams tokens in the following format, token_string:integer_index. Words in the vocabulary must be sorted in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write into a file (contains the bigrams and unigrams tokens)\n",
    "f = open('29286875_vocab.txt','a+')\n",
    "f.write(my_CVs)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countVec.txt\n",
    "29286875_countVec.txt: The txt file contains all the “selected” resumes in the data-set. Each line in the txt file contains the sparse representations of one of the resumes in the data-set in the following format file_name, token_index: count, token_index: count,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write into a file (contains all the “selected” resumes in the data-set)\n",
    "f = open('29286875_countVec.txt','a+')\n",
    "f.write(my_CVs)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    # -------------移除rare 2%\n",
    "    # Create a function for getting the percentage of each token frequency\n",
    "    def percentage(each, total):\n",
    "        return 100 * (each / total)\n",
    "\n",
    "    # Get distinct tokens remaining filtered from filtered_tokens\n",
    "    lexical_diversity = set(stemmed_tokens)\n",
    "\n",
    "    # Create an empty list to store tokens without rare (2%) ones\n",
    "    often_tokens = []\n",
    "    \n",
    "    temp = []\n",
    "    temp = ''\n",
    "    \n",
    "    # Check each distinct token\n",
    "    for each in lexical_diversity: \n",
    "        # Calculate the percentage of each token frequency\n",
    "#         token_ratio = percentage(stemmed_tokens.count(each), len(vocab))\n",
    "        temp += str(100*(stemmed_tokens.count(each)/len(vocab))) + \"\\n\"\n",
    "#         temp += str(stemmed_tokens.count(each)) + \"/\" + str(len(vocab)) + \"\\n\"\n",
    "\n",
    "#         temp.append(token_ratio)\n",
    "#         # if the token frequency(%) > 2%\n",
    "#         if token_ratio > 0.02:\n",
    "#             # Append into often_tokens\n",
    "#             often_tokens.append(each)\n",
    "#     # Sort often_tokens excluden\n",
    "#     sorted_often_tokens = sorted(often_tokens)\n",
    "\n",
    "#     # TEST: remove rare tokens (2%)\n",
    "    \n",
    "    return print(temp)\n",
    "#     return print(len(set(sorted_often_tokens)), len(set(stemmed_tokens)), len(set(vocab)))\n",
    "#     return print(sorted_often_tokens)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
