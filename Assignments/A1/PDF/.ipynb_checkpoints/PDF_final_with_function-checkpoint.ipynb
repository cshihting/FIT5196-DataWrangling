{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eileen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import miscellaneous operating system interfaces\n",
    "import os\n",
    "# library for regular expression\n",
    "import re\n",
    "# Import RegexpTokenizer (比較聰明的抓字)\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Import PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "# Extracting from a text a list of n-gram can be easily accomplished with function ngram()\n",
    "from nltk.util import ngrams\n",
    "# NLTK provides a built-in function FreqDist to compute this distribution directly from a set of word tokens.\n",
    "from nltk.probability import *\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.util import bigrams\n",
    "from nltk import BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset for me(29286875) copied from moodle\n",
    "myDataset = '262 76 778 565 6 623 200 742 645 418 520 814 467 680 216 12 762 63 \\\n",
    "276 66 236 722 82 779 626 206 276 649 750 20 167 738 602 6 251 808 \\\n",
    "729 115 80 71 539 38 426 44 556 779 251 760 61 728 251 268 421 750 \\\n",
    "832 807 55 452 683 303 560 555 547 265 649 441 633 108 517 744 137 \\\n",
    "446 182 397 442 8 266 630 857 17 498 32 293 711 413 445 392 606 322 \\\n",
    "422 54 582 657 239 309 382 286 105 336 786 788 712 738 516 29 523 \\\n",
    "51 428 43 47 819 649 598 297 615 208 865 720 122 789 372 860 813 \\\n",
    "219 700 275 4 466 455 277 427 728 71 812 284 646 327 397 262 22 \\\n",
    "450 80 377 26 332 45 325 24 274 153 847 311 168 99 361 362 139 602 \\\n",
    "551 517 111 812 1 364 467 323 398 207 837 716 733 685 498 817 336 \\\n",
    "395 154 265 492 117 389 833 705 703 270 218 435 475 338 513 621 \\\n",
    "208 839 560 370 81 445 89 12 645 336 157 47 91 32 797 837 803 195 \\\n",
    "861 237 31 810 634 211 246 278 788 556 726 684 815 460 596 237 640 \\\n",
    "764 251 304 474 154 384 743 749 593 327 749 515 543 310 833 464 724 \\\n",
    "66 331 649 397 426 383 492'\n",
    "\n",
    "# Create an empty list\n",
    "myDataset_ls = []\n",
    "\n",
    "# Convert string to list\n",
    "myDataset_ls = myDataset.split(' ')\n",
    "\n",
    "# Format filenames assign to cv_name_ls\n",
    "cv_name_ls = ['resume_(' + s + ').txt' for s in myDataset_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract bigrams\n",
    "def generate_collocations(tokens):\n",
    "    '''\n",
    "    Given list of tokens, return collocations.\n",
    "    '''\n",
    "\n",
    "    ignored_words = nltk.corpus.stopwords.words('english')\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "    # Best results with window_size, freq_filter of: (2,1) (2,2) (5,1)\n",
    "    finder = BigramCollocationFinder.from_words(tokens, window_size = 2)\n",
    "    finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "    finder.apply_freq_filter(1)\n",
    "\n",
    "    colls = finder.nbest(bigram_measures.likelihood_ratio, 20)\n",
    "\n",
    "    return colls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(filename, each_resume, stopword):\n",
    "    # -------------抓字\n",
    "    # Remove whitespace and join into a string\n",
    "    clean_resume = ' '.join(each_resume.split())\n",
    "    \n",
    "    # Set regex for each token\n",
    "    token_regex = r\"\\w+(?:[-']\\w+)?\"\n",
    "\n",
    "    # Find the matched token in clean_string\n",
    "    match_token = re.findall(token_regex, clean_resume)\n",
    "    \n",
    "    # -------------轉成小寫，除了中間的capital word\n",
    "    # Get capital tokens in the middle of the sentences by using regex\n",
    "    capital_tokens_regex = r'(?<!\\.\\s)(\\b[A-Z](?:[A-Z]*|[a-z]*)\\b|\\b[A-Z][a-z]+[A-Z][a-z]+\\b)'\n",
    "    \n",
    "    # Find all matched tokens in clean_string\n",
    "    match_capital = re.findall(capital_tokens_regex, clean_resume)\n",
    "    \n",
    "    # Filter two lists to get the same values\n",
    "    vocab = list(set(match_token) & set(match_capital))\n",
    "\n",
    "    # -------------移除stopword\n",
    "    # Create an empty list to store stopwords\n",
    "    stopwords_list = []\n",
    "\n",
    "    # Open and read the stopword file, then add into stopwords_list\n",
    "    with open(stopword) as f:\n",
    "        stopwords_list = f.read().splitlines()\n",
    "    \n",
    "    # Create a tokens list without stopwords\n",
    "    filtered_tokens = [token for token in vocab if token not in stopwords_list]\n",
    "\n",
    "    # -------------stem 轉換回根源\n",
    "    stemmer = PorterStemmer()\n",
    "    # Stem tokens using the Porter stemmer\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    # -------------移除rare 2%\n",
    "    # Create a function for getting the percentage of each token frequency\n",
    "    def percentage(each, total):\n",
    "        return 100 * (each / total)\n",
    "\n",
    "    # Get distinct tokens remaining filtered from filtered_tokens\n",
    "    lexical_diversity = set(stemmed_tokens)\n",
    "\n",
    "    # Create an empty list to store tokens without rare (2%) ones\n",
    "    often_tokens = []\n",
    "    \n",
    "    # Check each distinct token\n",
    "    for each in lexical_diversity: \n",
    "        # Calculate the percentage of each token frequency\n",
    "        token_ratio = percentage(stemmed_tokens.count(each), len(vocab))\n",
    "        # if the token frequency(%) > 2%\n",
    "        if token_ratio > 0.02:\n",
    "            # Append into often_tokens\n",
    "            often_tokens.append(each)\n",
    "    # Sort often_tokens excluden\n",
    "    sorted_often_tokens = sorted(often_tokens)\n",
    "    \n",
    "    # -------------移除三個字母以下的字\n",
    "    # Create an empty list\n",
    "    clean_uni_tokens = []\n",
    "\n",
    "    # Check each token in sorted_often_tokens\n",
    "    for each in sorted_often_tokens:\n",
    "        # If the length of each token >= 3\n",
    "        if len(each) >= 3:\n",
    "            # Append that token into token_3more_len_list\n",
    "            clean_uni_tokens.append(each)\n",
    "            \n",
    "    # -------------前200個有意義的bigrams\n",
    "    # Get the bigrams list like [('I', 'am'), ('good', 'day'), ...]\n",
    "    bigram_ls = generate_collocations(match_token)\n",
    "    \n",
    "    # Create an empty list\n",
    "    split_ls = []\n",
    "    # Get all values from bigram_ls and store into split_ls\n",
    "    for bigram in range(len(bigram_ls)):\n",
    "        for word in range(len(bigram_ls[bigram])):\n",
    "            split_ls.append(bigram_ls[bigram][word])\n",
    "    \n",
    "    # Create an empty list\n",
    "    best200 = []\n",
    "    # Make it look like [('I am'), ('good day'), ...]\n",
    "    for each in range(0, len(split_ls), 2):\n",
    "        best200.append(split_ls[each]+\" \"+split_ls[each+1])\n",
    "    \n",
    "#     return print(best200)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------vocab (token_string:integer_index)\n",
    "    final_tokens = clean_uni_tokens + best200\n",
    "    \n",
    "    vocab_str = ''\n",
    "    for integer_index, token_string in enumerate(final_tokens, 1):\n",
    "        vocab_str += token_string + \": \" + str(integer_index) + \"\\n\"\n",
    "        \n",
    "#     # Write into a file\n",
    "#     f = open('29286875_vocab.txt','a+')\n",
    "#     f.write(\"\\n-----------\\n\" + vocab_str + \"\\n\")\n",
    "#     f.close()    \n",
    "    \n",
    "    # 剩這裡了\n",
    "    # -------------countVec (file_name, token_index:count, token_index:count,...)\n",
    "    uni_count = []\n",
    "    bi_count = []\n",
    "    \n",
    "    unigram_diversity = set(match_token)\n",
    "    for uni in unigram_diversity:\n",
    "        uni_count.append(match_token.count(uni))\n",
    "        \n",
    "    \n",
    "#     bigram_diversity = set(best200)\n",
    "#     for bi in bigram_diversity:\n",
    "#         bi_count.append(match_token.count(bi))\n",
    "        \n",
    "#     count_tokens = []\n",
    "    \n",
    "    return print(uni_count)\n",
    "    \n",
    "#     # Write into a file\n",
    "#     f = open('29286875_countVec.txt','a+')\n",
    "#     f.write(vocab_str + \"\\n\")\n",
    "#     f.close() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 2, 1, 1, 9, 5, 2, 1, 1, 1, 3, 1, 1, 1, 34, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 6, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 4, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 5, 2, 2, 2, 2, 1, 3, 4, 2, 3, 3, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 6, 1, 4, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 8, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 5, 2, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 11, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 3, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 6, 1, 1, 1, 1, 1, 1, 2, 3, 1, 5]\n",
      "[1, 1, 1, 5, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 4, 1, 1, 3, 5, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 7, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 5, 1, 1, 2, 2, 1, 2, 4, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 12, 1, 1, 2, 1, 1, 4, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 8, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 5, 1, 3, 4, 1, 3, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1, 5, 1, 3, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 8, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 2, 1, 2, 1, 1, 1, 7, 6, 1, 1, 1, 2, 2, 1, 1, 1, 3, 2, 44, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 5, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 6, 2, 3, 1, 1, 2, 1, 4, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 3, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 4, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 8, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 2, 1, 1, 1, 2, 2, 2, 8, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 7, 2, 1, 1, 1, 3, 14, 1, 2, 1, 14, 1, 3]\n",
      "[1, 1, 1, 1, 1, 3, 1, 5, 2, 1, 1, 1, 3, 5, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 4, 1, 1, 2, 1, 3, 9, 2, 2, 1, 3, 2, 2, 3, 2, 4, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 5, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 4, 1, 2, 2, 1, 1, 1, 1, 1, 10, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 3, 4, 1, 1, 1, 2, 2, 2, 2, 3, 1, 1, 1, 1, 2, 5, 1, 2, 1, 8, 2, 1, 2, 1, 2, 4, 1, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 1, 31, 1, 1, 2, 1, 1, 1, 1, 5, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 5, 3, 1, 1, 2, 1, 2, 8, 1, 2, 2, 1, 2, 1, 2, 2, 3, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 5, 1, 3, 5, 1, 1, 3, 1, 1, 2, 2, 2, 1, 4, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 2, 3, 1, 2, 1, 1, 1, 1, 1, 3, 1, 2, 1, 2, 1, 1, 4, 1, 1, 3, 1, 2, 8, 3, 1, 1, 1, 1, 2, 1, 1, 1, 2, 3, 3, 1, 1, 1, 2, 1, 1, 5, 2, 1, 2, 1, 7, 10, 1, 1, 2, 1, 1, 3, 2, 1, 1, 4, 2, 4, 4, 2, 76, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 2, 1, 1, 3, 2, 1, 2, 2, 1, 7, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 4, 2, 1, 2, 2, 1, 3, 1, 1, 1, 2, 10, 2, 4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 2, 2, 4, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 3, 1, 3, 4, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 5, 1, 1, 2, 2, 1, 1, 1, 1, 3, 2, 3, 1, 1, 1, 4, 1, 2, 1, 2, 1, 1, 3, 1, 4, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 3, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 4, 1, 1, 5, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 2, 1, 1, 2, 8, 1, 1, 3, 1, 2, 1, 1, 2, 3, 1, 8, 1, 1, 2, 4, 3, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 2, 8, 1, 1, 1, 3, 2, 2, 2, 1, 1, 1, 4, 2, 2, 9, 1, 1, 1, 1, 1, 3, 5, 1, 1, 1, 1, 2, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 14, 2, 2, 1, 2, 1, 1, 1, 1, 6, 24, 1, 2, 2, 2, 1, 1, 2, 28, 1, 2, 6]\n",
      "[1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 2, 3, 5, 2, 4, 1, 1, 4, 1, 3, 1, 3, 2, 5, 5, 2, 2, 1, 1, 2, 1, 1, 14, 2, 3, 1, 1, 6, 1, 4, 1, 3, 1, 2, 2, 3, 3, 9, 1, 2, 2, 1, 1, 1, 1, 3, 3, 2, 4, 2, 4, 2, 3, 1, 1, 1, 1, 1, 2, 1, 3, 1, 2, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 3, 8, 1, 4, 1, 3, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 6, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 5, 1, 6, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 2, 6, 1, 2, 2, 10, 1, 1, 1, 2, 3, 6, 1, 1, 8, 7, 3, 12, 1, 1, 2, 1, 1, 2, 1, 3, 2, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 3, 1, 6, 1, 1, 3, 1, 4, 4, 1, 6, 2, 1, 1, 1, 1, 1, 6, 2, 3, 1, 1, 1, 1, 1, 2, 1, 1, 6, 1, 1, 1, 1, 3, 4, 2, 1, 3, 1, 1, 1, 1, 2, 2, 2, 2, 1, 3, 3, 1, 1, 1, 1, 1, 3, 1, 2, 1, 2, 5, 1, 1, 2, 3, 1, 1, 2, 1, 8, 2, 1, 2, 1, 5, 1, 2, 9, 1, 4, 2, 2, 1, 7, 1, 12, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 2, 1, 1, 5, 1, 65, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 4, 1, 5, 1, 2, 1, 1, 1, 2, 2, 1, 3, 1, 3, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 5, 3, 4, 1, 2, 3, 2, 9, 1, 8, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 3, 3, 2, 1, 1, 1, 2, 1, 3, 1, 7, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 4, 3, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 4, 1, 1, 1, 4, 1, 1, 1, 3, 5, 4, 1, 2, 5, 1, 1, 3, 5, 1, 1, 1, 1, 1, 3, 2, 1, 1, 4, 2, 2, 2, 1, 1, 1, 4, 2, 3, 2, 1, 1, 2, 1, 1, 1, 3, 3, 2, 1, 1, 2, 1, 1, 1, 3, 2, 2, 2, 3, 2, 1, 1, 6, 1, 1, 1, 3, 6, 2, 1, 1, 4, 4, 1, 1, 1, 3, 3, 1, 6, 1, 1, 1, 1, 9, 1, 2, 3, 3, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 3, 4, 5, 2, 1, 6, 1, 2, 2, 1, 1, 5, 1, 1, 2, 3, 1, 2, 11, 4, 1, 1, 1, 3, 4, 1, 1, 2, 1, 1, 1, 3, 2, 1, 1, 1, 1, 2, 3, 5, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 10, 1, 1, 1, 1, 5, 1, 1, 2, 1, 1, 1, 2, 2, 1, 9, 1, 16, 1, 1, 6, 1, 1, 1, 1, 3, 14, 1, 3, 2, 1, 1, 1, 1, 4, 3, 4, 4, 1, 7, 106, 1, 1, 2, 1, 4, 1, 1, 1, 1, 1, 3, 8, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 3, 2, 1, 1, 1, 1, 5, 1, 2, 1, 2, 1, 2, 1, 1, 1, 4, 4, 7, 1, 3, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 5, 2, 1, 1, 2, 1, 4, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 3, 1, 5, 1, 1, 2, 1, 10, 1, 3, 2, 1, 4, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 1, 4, 8, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 7, 2, 1, 13, 2, 1, 2, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 4, 1, 7, 1, 1, 1, 2, 5, 2, 1, 2, 1, 5, 1, 2, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 7, 4, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 5, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 2, 1, 5, 1, 1, 1, 1, 5, 1, 2, 2, 1, 1, 2, 1, 1, 1, 7, 1, 1, 1, 4, 1, 4, 1, 3, 1, 2, 1, 1, 1, 3, 3, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 5, 2, 1, 2, 3, 2, 2, 1, 2, 1, 2, 1, 1, 1, 4, 1, 1, 2, 2, 1, 1, 1, 1, 3, 4, 4, 1, 1, 5, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 2, 6, 2, 1, 2, 1, 2, 3, 2, 2, 1, 3, 1, 1, 1, 3, 2, 9, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 3, 3, 1, 1, 8, 1, 1, 2, 1, 4, 1, 1, 3, 1, 5, 1, 3, 1, 1, 1, 2, 4, 3, 1, 1, 1, 2, 21, 1, 1, 6, 1, 5, 2, 12, 1, 2, 1, 1, 2, 3, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 4, 2, 2, 17, 5, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 5, 1, 1, 1, 11, 1, 1, 1, 2, 1, 3, 2, 1, 1, 4, 1, 3, 3, 1, 1, 1, 1, 1, 1, 2, 1, 22, 2, 2, 2, 1, 2, 1, 8, 1, 1, 3, 2, 1, 2, 1, 1, 1, 7, 57, 1, 2, 1, 1, 3, 2, 1, 2, 2, 2, 2, 1, 1, 38, 1, 1, 2, 1, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "# Import miscellaneous operating system interfaces\n",
    "import os\n",
    "\n",
    "# Create an empty string\n",
    "per_CV = ''\n",
    "my_CVs = ''\n",
    "\n",
    "# Return a list containing the names of the entries in the directory given by path\n",
    "# os.listdir(path of resumeTxt)\n",
    "for file in os.listdir(\"/Users/eileen/Jupyter/5196 data wrangling/Assignments/A1/PDF\"):\n",
    "    # If the file is in cv_name_ls\n",
    "    if file in cv_name_ls:\n",
    "        # Open and read that file from the chosen path\n",
    "        with open(file, 'r') as f:\n",
    "            # Read line by line\n",
    "            for line in f.readlines():\n",
    "                # Add into my_CVs\n",
    "                per_CV += line\n",
    "            # Function for analyzing\n",
    "            analyze(file, per_CV, 'stopwords_en.txt')\n",
    "\n",
    "# Write into a file (test)\n",
    "# f = open('29286875_CVs.txt','a+')\n",
    "# f.write(my_CVs)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
